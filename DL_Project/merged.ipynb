{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS236781: Deep Learning on Computational Accelerators\n",
    "# Homework Assignment 4\n",
    "\n",
    "Faculty of Computer Science, Technion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted by:\n",
    "\n",
    "| #       |              Name |             Id |             email |\n",
    "|---------|-------------------|----------------|------------------ |\n",
    "|Student 1|  [Murad Ektilat] | [] | [muradek@campus.technion.ac.il] |\n",
    "|Student 2|  [Ariel Sernoff] | [] |[arielsernoff@campus.technion.ac.il]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we'll explore deep reinforcement learning.\n",
    "We'll implement two popular and related methods for directly learning the policy of an\n",
    "agent for playing a simple video game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Guidelines\n",
    "\n",
    "- Please read the [getting started page](https://vistalab-technion.github.io/cs236781/assignments/getting-started) on the course website. It explains how to **setup, run and submit** the assignment.\n",
    "- This assignment requires running on GPU-enabled hardware. Please read the [course servers usage guide](https://vistalab-technion.github.io/cs236781/assignments/hpc-servers). It explains how to use and run your code on the course servers to benefit from training with GPUs.\n",
    "- The text and code cells in these notebooks are intended to guide you through the\n",
    "  assignment and help you verify your solutions.\n",
    "  The notebooks **do not need to be edited** at all (unless explicitly specified).\n",
    "  The only exception is to fill your name(s) in the above cell before submission.\n",
    "  Please do not remove sections or change the order of any cells.\n",
    "- All your code (and even answers to questions) should be written in the files\n",
    "  within the python package corresponding the assignment number (`hw1`, `hw2`, etc).\n",
    "  You can of course use any editor or IDE to work on these files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Part1: Mini-Project](#part1)\n",
    "- [Part2: Summary Questions](#part2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "$$\n",
    "\n",
    "# Part 1: Mini-Project\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "#### Creating the Dataset\n",
    "###### Downloading the data\n",
    "###### Data Directory Preprocessing\n",
    "###### Datasets creation with the Roboflow API\n",
    "\n",
    "#### Creating the Model\n",
    "###### YOLOv8\n",
    "###### Model Architecture\n",
    "###### Loss function\n",
    "###### Optimization\n",
    "\n",
    "#### Evaluating the Model\n",
    "###### Predicting on the test set\n",
    "###### COCO evaluation\n",
    "\n",
    "#### Results and Discussion\n",
    "###### Training and Validation results\n",
    "###### yolo test results\n",
    "###### coco evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating the Datasets\n",
    "\n",
    "### Downloading the data\n",
    "As instructed for the project, we've used the stable version of TACO dataset which consists of 1500 images and 4787 annotations.\n",
    "The images were downloaded from the https://github.com/pedropro/TACO repository.\n",
    "The annotations were downloaded from https://github.com/wimlds-trojmiasto/detect-waste/tree/main/annotations repository.\n",
    "We've used the annotations_train.json and annotations_test.json files, including 7 detect-waste categories for the object detection task:\n",
    "- bio\n",
    "- glass\n",
    "- metals_and_plastic\n",
    "- non_recyclable\n",
    "- other\n",
    "- paper\n",
    "- unknown\n",
    "\n",
    "### Data Directory Preprocessing\n",
    "We used the Roboflow API to create our datasets. In order to use the Roboflow API we've implemnted a python script that edits the data directory in two steps (the script is ran in the next cell).\n",
    "1. Flattening the directory: The original structure of the directory had 15 sub-directories (batches 1-15) each containing ~100 images, each image named as a number in [1, ~100]. We flattend the subdirectories to one directory, and changed the images names, adjusting the corresponding images names in the annotations files.\n",
    "2. Splitting the directory into two: Test_dir and Train_dir, based on the images partition in the annotation files.\n",
    "\n",
    "### Datasets creation with the Roboflow API\n",
    "Once the data directory was prepared, we've uploaded it to Roboflow and created two seperated datasets.\n",
    "1. A training set with 1182 images (~79% of the data). We split this dataset to two subsets: Train set (1062 images, 90%) and a validation subset (118 images, 10%).\n",
    "2. A test set with 317 images. (~21% of the data).\n",
    "\n",
    "With the roboflow API we've processed the images with the following tools:\n",
    "1. resized all training images to 640x640.\n",
    "2. applied auto-orientation to correct mismatchs between annotations and images.\n",
    "\n",
    "Then We created the dataset, which can be downloaded using the code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from roboflow import Roboflow\n",
    "# from ultralytics import YOLO\n",
    "# import project.model_training as mt\n",
    "\n",
    "# preprocess data directory to fit roboflow\n",
    "# the script is commented as the data location in the server may vary. \n",
    "# [it is said to be in `datasets/TACO-master` but we could not find it there]  \n",
    "\n",
    "# %run project/preprocess_imgs.py\n",
    "\n",
    "# upload the processed data to roboflow as explained above\n",
    "\n",
    "# download the datasets\n",
    "# train_set, test_set = mt.load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating the Model\n",
    "### YOLOv8\n",
    "We chose to approach the task by custom training the YOLOv8 model. This model is regarded as one of the leading models in image classification, detection and segmentation. To achive best results, we've used the largest, most accurate version of the model (YOLOv8x). We trained our model for a 100 epochs, taking into considaration 3 main factors:\n",
    "1. Maximizing validation result during the training process\n",
    "2. Avoiding overfitting to the training set\n",
    "3. least important factor, but still realevent: Costâ€“benefit analysis for time and resources consumption during training the model [i.e: with more time and resources, it is possible to run more training sessions, perform cross-validation etc to reach better results]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing and training the model.\n",
    "# This code block is commented so that the notebook wont preform training.\n",
    "# To re-create our the training process, uncomment and run the next line\n",
    "\n",
    "# model, train_res = mt.set_model(train_set, 'yolov8x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To avoid training, load our trained model:\n",
    "# model = YOLO(\"runs/detect/train27/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Architecture:\n",
    "YOLO V8 consists of two main components. A backbone and a head. The backbone is a series of convolutional networks and course to fine (C2f) layers. The backbone creates features which are then passed to the head for detection using the models loss function. A diagram by [RangeKing](https://github.com/RangeKing) of the model can be seen here.\n",
    "\n",
    "<div>\n",
    "<img src=\"imgs/yolov8_architecture_diagram.jpeg\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "Sublayers are included in the diagram and it illustrates each well.\n",
    "\n",
    "The architecture utilizes bottlenecks and a pyramidal structure for the architecture. One pyramidal concept is the spatial pyramid pooling layers (SPP/SPPF).\n",
    "\n",
    "Some changes in this version of YOLO include;  \n",
    "\n",
    "    - Not using anchor boxes for detection which increased speed.\n",
    "    \n",
    "    - A new backbone consisting of new convolutional building block and new C2f layers which have additional residual connections.\n",
    "    \n",
    "    - And new loss functions\n",
    "    \n",
    "The full model can be seen here on the [YOLOv8 repo](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/models/v8/yolov8.yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function:\n",
    "\n",
    "The model uses a loss function that combines several elements to measure the total loss.\n",
    "\n",
    "- The first part is a Bbox Loss. The bbox loss returns two seperate loss values. The Bbox loss holds a componenet of the total loss that measures and evaluates the loss of the bounding boxes generated by the model.  \n",
    "\n",
    "1. IoU Loss: Which is a standard intersection over union loss. Calculated by using an external bbox_iou method.\n",
    "\n",
    "2. DFL Loss: Which is a distributional focal loss function. As proposed in this [paper](https://ieeexplore.ieee.org/document/9792391). In short this is a loss function that also measures the quality of box locations but does so using distribution based methods. \n",
    "\n",
    "Below is the code of the Bbox loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class BboxLoss(nn.Module):\n",
    "\n",
    "#     def __init__(self, reg_max, use_dfl=False):\n",
    "#         super().__init__()\n",
    "#         self.reg_max = reg_max\n",
    "#         self.use_dfl = use_dfl\n",
    "\n",
    "#     def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n",
    "#         # IoU loss\n",
    "#         weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
    "#         iou = bbox_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask], xywh=False, CIoU=True)\n",
    "#         loss_iou = ((1.0 - iou) * weight).sum() / target_scores_sum\n",
    "\n",
    "#         # DFL loss\n",
    "#         if self.use_dfl:\n",
    "#             target_ltrb = bbox2dist(anchor_points, target_bboxes, self.reg_max)\n",
    "#             loss_dfl = self._df_loss(pred_dist[fg_mask].view(-1, self.reg_max + 1), target_ltrb[fg_mask]) * weight\n",
    "#             loss_dfl = loss_dfl.sum() / target_scores_sum\n",
    "#         else:\n",
    "#             loss_dfl = torch.tensor(0.0).to(pred_dist.device)\n",
    "\n",
    "#         return loss_iou, loss_dfl\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _df_loss(pred_dist, target):\n",
    "#         # Return sum of left and right DFL losses\n",
    "#         # Distribution Focal Loss (DFL) proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n",
    "#         tl = target.long()  # target left\n",
    "#         tr = tl + 1  # target right\n",
    "#         wl = tr - target  # weight left\n",
    "#         wr = 1 - wl  # weight right\n",
    "#         return (F.cross_entropy(pred_dist, tl.view(-1), reduction='none').view(tl.shape) * wl +\n",
    "#                 F.cross_entropy(pred_dist, tr.view(-1), reduction='none').view(tl.shape) * wr).mean(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second part is a Varifocal loss which gives a classification loss component to the total loss. It is defined in this [paper](https://arxiv.org/pdf/2008.13367.pdf) as:  \n",
    "\n",
    "<div>\n",
    "<img src=\"imgs/VFL3.png\" width=\"500\"/>\n",
    "</div>\n",
    "<div>\n",
    "<img src=\"imgs/VFL2.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Which is a take on binary cross entropy and is further explained in detail in the paper. In general focal losses help classify when we have imbalanced classes. Where some examples are easily classified and others are more difficult, the loss then focuses more on the challenging examples. In general this is a strong classification loss function.\n",
    "\n",
    "We can see that the code of the loss function also includes an existing binary cross entropy method: binary_cross_entropy_with_logits\n",
    "\n",
    "Which from its documentation is a combination of binary cross entropy with a sigmoid layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class VarifocalLoss(nn.Module):\n",
    "#     # Varifocal loss by Zhang et al. https://arxiv.org/abs/2008.13367\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, pred_score, gt_score, label, alpha=0.75, gamma=2.0):\n",
    "#         weight = alpha * pred_score.sigmoid().pow(gamma) * (1 - label) + gt_score * label\n",
    "#         with torch.cuda.amp.autocast(enabled=False):\n",
    "#             loss = (F.binary_cross_entropy_with_logits(pred_score.float(), gt_score.float(), reduction='none') *\n",
    "#                     weight).sum()\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization: \n",
    "\n",
    "The YOLOv8 model uses a default optimizer of ADAM.\n",
    "ADAM is an extented version of stochastic gradient decent with momentum that only uses first order gradients andÂ \n",
    "is based on adaptive estimates of lower-order moments. Empirical results show that adam produces good results in comparison to other\n",
    "optimizer algorithms and is well suited for large data.\n",
    "the default hyper parameters in the model are: Learning rate=0.001, Momentum=0.9, Decay=1e-5\n",
    "\n",
    "We choose to use this optimizer relying on the fact that ADAM is a SOTA optimization algorithim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "### Predicting on the test set\n",
    "After we trained the model, we used the YOLO.val() method to predict on our test set.\n",
    "The method performs object detection on our unseen test set images.\n",
    "To deeper our model evaluation, we've performed another analysis using the cocotools. \n",
    "To perform the evaluation, we compared the ground truth annotation file and the detected annotations for the test dataset.\n",
    "The detected annotation file is created by the VAL() function, and processed by our next script to fit the cocoEVAL() comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# edit the test_set to fit evaluations\n",
    "# %run project/edit_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict with the model on the test set for evaluation\n",
    "# test_res = mt.evaluate_model(test_set, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform the COCO evaluation\n",
    "# %run project/cocoEval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results and dicussion\n",
    "### Training and Validation results\n",
    "During the training process, the model validates its performence after each epoch on the validation subset.\n",
    "- Reminder & Clarification: Our validation subset is part of the training Dataset but the model does not train on it explicitly. This means that the validation subset does not affect the weights directly. However, as we use those results to manualy tune the model's version, size and hyperparameters, the validation subset is *not* used for the test results (which we'll discuss later on). \n",
    "\n",
    "In the following graph, we can see the model's performence as a function of the training epochs.\n",
    "<div>   \n",
    "<img src=\"imgs/results_train.png\" width=\"800\"/>  \n",
    "</div>\n",
    "\n",
    "* Few important distinctions:\n",
    "1. As expected, we can see that during the training process the loss values (box, cls, dfl) decrease and the precision values increase for both training and validation set.\n",
    "2. We see that the graphs did not reach a plateau yet, indicating we might still be able to improve the model performence. After severall training processes, we've noticed that we can still improve the model's precision on the training set, but this will cause overfitting effect for some of the categories. Therfore we limited the model epochs. \n",
    "\n",
    "In the following figure, we can observe the different predictions disribution broken into categories.\n",
    "<div>   \n",
    "<img src=\"imgs/matrix_train.png\" width=\"800\"/>  \n",
    "</div>\n",
    "\n",
    "* We can see that there are two categories that are \"overpredicted\"(False Positive - FP): background and metals_and_plastic.\n",
    "We can lower the background FP value by decreasing the confidence threshold for the detection. However, this lowers the overall precision as it increases the FN values, and specifically increases the metals_and_plastic FP value.\n",
    "\n",
    "* Another distinction is that the model rarely predicts the \"other\" category. We assume that this happens due to the fact that as opposed to the rest of the cattegories, \"other\" has no dintinct definition. therfore the model cant find unique patterns (weights) to detect it.\n",
    "\n",
    "* The 'bio' column is empty as there are no bio labels in the validation set (generally, there are very few bio labels in the TACO dataset, hence we expect that the model wont detect them well or wont detect them at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO test results\n",
    "\n",
    "<div>   \n",
    "<img src=\"imgs/matrix_test_3.png\" width=\"800\"/>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the training results we can again notice the over-prediction of background and metals_and_plastics which is not surprising having been a difficulty of the model during training.\n",
    "\n",
    "There were no major changes in comparison to the training. However we can notice some added confusion between glass and non-recyclables and unknown and paper. The changes are not major and we can still observe similar results.\n",
    "\n",
    " When taking into account the large variability of the data set and the fact that we trained a large model and likely slightly overfitted the data to some inevitable extent the results are reasonable. It is worth noting that in general the entire YOLOv8 model barely reaches over 50 mAP on the COCO 2017 image dataset. Considering that the dataset is substantially smaller and that we had less time and resources to train the model on it, we think the results are decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### COCO evaluation results\n",
    "\n",
    "<div>   \n",
    "<img src=\"imgs/coco_eval.png\" width=\"600\"/>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see that the results are similar to the results produced in the training set. One thing worth noting is that with the final Average recall with IoU=0.5:095 and large area we reach an Average recall of 0.13 which is decent recall value when taking into consideration the limitations of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\renewcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\renewcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\renewcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\renewcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\renewcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\renewcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\renewcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\renewcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\renewcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\renewcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\renewcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\renewcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\renewcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\renewcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\renewcommand{\\grad}[]{\\nabla}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$\n",
    "\n",
    "# Part 2: Summary Questions\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The receptive field in context of CNNs refers to the portion/region of the previous layer in the NN that needs to be used in order to calculate the next feature. There is a closed formula for this: \n",
    "$$r_{l-1}=s_l \\cdot r_l + (k_l - s_l)$$ \n",
    "Where $l$ is the index of the current layer, $r_l$ is the receptive field of the lth layer, $k_l$ is the kernel size, $s_l$ is the stride of layer l and $d_l$ is the dilation of layer L. \n",
    "Thus the equation can be further developed arithmetically by solving the recurrence equation and express the receptive field of the network, $r_0$ as follows:\n",
    "$$r_0 = \\sum_{l=1}^{L}\\Bigl(d_l(k_l - 1) \\prod_{i=1}^{l-1} s_i\\Bigl) +1$$\n",
    "The receptive field is used as a measure to understand how much of the initial input is used when computing an output. In general a larger receptive field indicates that more complex or global features are being used. For example, in image detection/classification it can indicate the complex features of the entire image are used and not just smaller/local patterns. \n",
    "\n",
    "TODO: double check calculations with regard to dilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Three ways to control the rate at which the receptive field grows are: Pooling, strides and dilation. Also it worth mentioning that trivially is we increase the kernel size the RF will also grow.\n",
    "\n",
    "Pooling is adding adding a pooling layer to the networks architecture, this pooling layer consists of a pooling function which in general replaces the output of the network in that location with a summary statistic of nearby outputs. For example we can look at a max pooling function which returns the maximum output within a specified area (usually a rectangle) in the area. This also helps improve NNs because it can add invariance to small translations in the input. In terms of receptive field (RF) this can help improve the NN by increasing the RF because it considers the inputs neighbors when producing the output of the layer. Thus we can control the RF by increasing or decreasing the pooling functions we choose to use. In general as we increase the window of the pooling we increase the RF and conversely when decreasing.\n",
    "\n",
    "Stride is the spatial distance that a convolution filter moves/steps in between consecutive applications of the filter the kernel. Informally it can be regarded as the number of cells that the filter moves vertically/horizontally between each convolution operation. A large stride will thus produce a smaller output of that convolution layer because the filter will complete the operations on the output with less applications. If we choose a small stride we will receive a larger RF and opposite if we use a large stride. \n",
    "\n",
    "Dilation is the spacing between kernel elements when applying to input. Informally it adds gaps/holes in the filter. This spaces out the area the convolution layer sees and thus increases the receptive field. However we can notice that since the kernel is simply spread out and not otherwise altered, we gain an increase in the RF without adding new parameters to the NN. By increasing the dilation we increase the RF and vice versa. This method can help capture large-scale features. \n",
    "\n",
    "When comparing the three methods and how they combine input features we can note that Pooling comes at the expense of adding an additional layer thus further adding parameters to our NN when increasing the receptive field. A small stride will increase the RF but also increase the parameters of the NN because the output layer  of the convolution will have more neurons. Dilation on the other hand has no effect on the number or parameters and when increased will increase the RF too. \n",
    "\n",
    "In terms of how they combine input features: Pooling usually takes some sort of norm of the pooling area and thus can loose spatial resolution of the network, but this helps the NN deal with small changes in the input and helps prevent over fitting. Stride applies the convolution function on a smaller number of elements thus can reduce computational cost and make the NN more efficient. Dilation will combine the input features in a larger-scale way and thus can create non conventional and global features and improve efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 122, 122])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "First we can note that since all the max pool layers have a kernel size and stride of two they will double the receptive field.\n",
    "Next we can calculate the size of the receptive field through each layer using the closed formula and accounting for the last layers dilation. \n",
    "Note that the Relu Layers do not effect the RF and thus we can exclude the from the calculation.\n",
    "\n",
    "Explicitly we get the following:\n",
    "$$r_5 = d_5\\cdot s_5(k_5-1)+1 = 2 \\cdot 1 \\cdot 6 +1 = 13$$\n",
    "$$r_4 = r_5 \\cdot 2 = 26 $$\n",
    "$$r_3 = d_3 \\cdot s_3 (r_4-1) + k_3 = 1 \\cdot 2 (26-1) + 5 = 55$$\n",
    "$$r_2 = r_3 \\cdot 2 = 110 $$\n",
    "$$r_1 = d_1 \\cdot s_1 (r_2-1) + k_1 = 1 \\cdot 1 (110-1) + 3 = 112$$\n",
    "And using the closed formula: \n",
    "$$ d_5(k_5-1)(s_4 \\cdot s_3 \\cdot s_2 \\cdot s_1) + (k_4-1)(s_3 \\cdot s_2 \\cdot s_1) + d_3(k_3-1)(s_2 \\cdot s_1) + (k_2-1)(\\cdot s_1) + d_1(k_1-1)(1) +1 = 2(7-1)(2 \\cdot 2 \\cdot 2 \\cdot 1) + (2-1)(2 \\cdot 2 \\cdot 1) + (5-1)(2 \\cdot 1) + (2-1)(1) + (3-1)(1) + 1 = 96+4+8+1+2+1 = 112$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We can see that with the residual mapping we receive a new mapping that references the input given to that layer. This is the implementation of ResNet in general. The reason that the residual network produces completely different filters is because now that the network can reference the input which is like adding additional information or a use of \"memory\" when constructing features. Thus the residual network extracts features that also regard the residual mapping that was given to it, which can lead to very different filters. ResNets usually correct the vanishing gradient issue and allow to build a much deeper network which can also lead to the different features extracted and filters constructed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Consider the following neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.1, inplace=False)\n",
       "  (3): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "p1, p2 = 0.1, 0.2\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=p1),\n",
    "    nn.Dropout(p=p2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to replace the two consecutive dropout layers with a single one defined as follows:\n",
    "```python\n",
    "nn.Dropout(p=q)\n",
    "```\n",
    "what would the value of `q` need to be? Write an expression for `q` in terms of `p1` and `p2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The parameter p passed to the dropout layer represents the probability that the element in the tensor (feature) will be zeroed. This has been proven to be an effective method to prevent overfitting by adding stochasticity to the model. Then if we regard the two events of dropout as independent (they are consecutive) we can infer from basic statistic rule that q need to equal the probability that feature is dropped in the first or second dropout layer. We can calculate this by calculating the probability of not being dropped in both layers and then calculating the chance of that not happening. Then:\n",
    "$$1-q = (1-p_1) * (1-p_2) \\Rightarrow q = 1 - (1-p_1) * (1-p_2) = 1 - 0.9 * 0.8 = 1-.72 = 0.28$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **True or false**: dropout must be placed only after the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** False. You can place dropout before or after an activation function. First with some activation functions there is no difference (for example, ReLU). In some cases, placing dropout layers before specific activation functions can improve the stability of the network if the input to the activation function has very large variance and the activation function amplifies this variance, then dropping out some values can reduce this effect. However it is always dependant on the specific case and in some instances it may be beneficial to place it before, for example if the activation function behaves in a detrimental way on values of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The issue arises due to the fact that when using the dropout regularization method we only wish to drop certain nodes in the network during training and retain the full network during testing. When we apply the drop out we alter the expected value of the weight matrices of training and to compensate for this alteration we thus need to scale the activations. \n",
    "\n",
    "Proof: \n",
    "We will assume under contradiction that scaling does not need to be done and will show that the expected value of the new activations is not equal to the old expected value without drop out, thus showing that to hold accuracy during testing we need to scale.\n",
    "\n",
    "Let $y_{i}$ be the vector of activations in the i'th node in a layer and $E[y_i]$ be the expected value of the activation and $E[y]$ is then the expected value of the entire vector of activations. \n",
    "\n",
    "After dropout is used on the layer the values are now defined:\n",
    "\n",
    "let $d_i$ be a random variable deciding if the node will be activated or not. i.e. it is an indicator with probability of $1-p$. Note that $d_i$ is independent of $y_i$. Then, \n",
    "\n",
    "$$y_{i}^{'} =  \\begin{cases}\n",
    "      y_i, & \\text{if}\\ d_i=1 \\\\\n",
    "      0, & \\text{if}\\ d_i=0\n",
    "    \\end{cases}$$\n",
    "\n",
    "Then when calculating the expectation of the whole layer we get. \n",
    "\n",
    "$$E[y^{'}] = \\sum_{i=0}^{n} E[y^{'}_{i}] = \\sum_{i=0}^{n} E[y_i \\cdot d_i] + E[0 \\cdot (1-d_i)] \\\\\n",
    "= \\sum_{i=0}^{n} E[y_i \\cdot (1-p)] + E[0 \\cdot (p)] = (1-p)E[y]$$\n",
    "\n",
    "Which is contradiction to our premise that we can use drop out without regularization without scaling because this leads to a different expected value of the activation vectors between training and testing. \n",
    "\n",
    "Thus to achieve equal expectation we must scale by a factor of $\\frac{1}{(1-p)}$ leading to,\n",
    "\n",
    "$$y_{i}^{'} =  \\begin{cases}\n",
    "      \\frac{1}{(1-p)}\\cdot y_i, & \\text{if}\\ d_i=1 \\\\\n",
    "      0, & \\text{if}\\ d_i=0\n",
    "    \\end{cases}$$\n",
    "\n",
    "$$E[y^{'}] = \\sum_{i=0}^{n} E[y^{'}_{i}] = \\sum_{i=0}^{n} E[\\frac{1}{(1-p)}\\cdot y_i \\cdot d_i] + E[0 \\cdot (1-d_i)] \\\\\n",
    "= \\sum_{i=0}^{n} E[\\frac{1}{(1-p)}\\cdot y_i \\cdot (1-p)] + E[0 \\cdot (p)] = E[y] $$\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You're training a an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** No, the benefits of an L2 loss are that it provides a weighted loss based on how \"far\" the output if from the true expected value. In classification we usually implement our models using a soft-max activation (or some  function returning a probability) in the last layer to extract the class that the network finds most probable to be the true label/class. In this case it doesn't matter how \"close\" the classifier was to the true label if it was mistaken, it classifies as the class with highest probability. L2 does not regard the fact that the output is a probability. In that sense we don't want to give weight to the classes and instead \"punish\" an incorrect classification equally regardless of how close it was. L2 loss is more suited for regression problems and problems with continuous outputs.  \n",
    "\n",
    "As a numerical example we can look at these two prediction vectors and notice how using the L2 norm as a loss function yields a smaller loss on the second vector even though the classification would be wrong. \n",
    "\n",
    "Ground Truth = [1, 0, 0, 0]  \n",
    "\n",
    "V1 = [0.3, 0.25, 0.25, 0.2] , L2 = 0.168  \n",
    "\n",
    "V2 = [0.49, 0.5, 0.01, 0.0] , L2 = 0.128  \n",
    "\n",
    "A more fitting loss function would be binary cross-entropy loss. Binary and also regular cross-entropy measure the difference between the predicted and true probability distributions, thus it takes in account the probabilistic nature of classification. \n",
    "\n",
    "Another reason why L2 loss is not optimal compared to binary cross-entropy loss is that L2 is sensitive to outliers where cross-entropy is less sensitive to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/PiratesVsTemp%28en%29.svg/1200px-PiratesVsTemp%28en%29.svg.png?20110518040647\" /></center>\n",
    "\n",
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H), nn.Sigmoid(),\n",
    "    ]*24,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** It is most likely due to the fact that the model is very large and that we are using the sigmoid activation function. The sigmoid activation function saturates at large positive and negative values. The sigmoid function has non-steep gradients and suffers heavily from the vanishing gradient problem, especially in cases like this with deep and wide networks with many hidden layers. The biggest gradient of the sigmoid function is 0.25 at $x=0$ and we can see by looking at the functions derivative that it would quickly have a gradient that limits to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** He is incorrect. The tanh function does in fact handle gradients better than the sigmoid, due to the greater derivatives of the function, but it also suffers from the vanishing gradient problem. The tanh activation function is often used in hidden layers so in this case because the majority of the model lies in one bulk of hidden layers it may work since it leaves the gradients between -1 and 1. However since the tanh function is also a type of sigmoidal function it often can act similarly to the sigmoid and most likely wont solve the issue. Usually using a ReLU or similar type of activation function is recommended. Also batch-normalization and proper weight initialization may help the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain:\n",
    "  1. In a model using exclusively ReLU activations, there can be no vanishing gradients.\n",
    "  1. The gradient of ReLU is linear with its input when the input is positive.\n",
    "  1. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "5. False: ReLU activation functions can suffer from something similar to the vanishing gradient issue and it is often coined the dying ReLU problem. The ReLU function gives a gradient of 0 to all negative and zero values. Which makes those nodes no longer continue the update the weights and stop responding to the training. In deep networks this can lead to a difficulty in training the model properly and reaching good results.\n",
    "\n",
    "6. False: First the ReLU is not differentiable at 0. Furthermore the gradient of the ReLU function is 0 for all negative values and 1 for all positive values. Thus we do not get a linear function due to the discrete case nature of the gradient and the original function. If the intended is not including zero then the answer is *True* because the gradient will be 1 for all values. \n",
    "\n",
    "7. True: That is precisely the dying ReLU problem noted above. All neurons that get a negative weight value will no longer update due to the gradient of the ReLU function being 0 for negative values. This will stop the backpropogation updating of those neurons. This is often attempted to be overcome by using Leaky Relu, ELU or similar activation functions. Also this can be dealt with in part with proper weight initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** GD, SGD and mini-batch SGD (mbSGD) are all similar optimization algorithms used to optimize the weights of a network during training. The main difference between them is how many samples they regard while computing the next step. To elaborate, all of the methods take a number of samples, calculate the gradient of the samples and take a step in the steepest direction (greatest gradient). While GD calculates the gradients for all samples in the training set, SGD approximates the step direction based of of one sample, and mbSGD is a hybrid of both methods that takes a sub-sample of the training set and calculates the direction to step based of all the gradients in the subgroup. The pay off is like in most cases; efficiency over accuracy. Calculating the gradients for all samples can prove to be very computationally exhausting and can even lead to local minimas. The stochastic methods are much more efficient in most cases and can often lead out of local minimas. However, it has been proven that the local minima issue in most cases is not as large of an issue as often perceived and that the difference between the local and global minimas is often not very substantial.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding SGD and GD:\n",
    "  1. Provide at least two reasons for why SGD is used more often in practice compared to GD.\n",
    "  2. In what cases can GD not be used at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: 3**  \n",
    "Reason 1: As stated above the computational efficiency of SGD often prevails over GD's accuracy in practice because GD calculates the gradients of all the training set while SGD only calculates the gradient of a single sample (or subset in mbSGD)\n",
    "\n",
    "Reason 2: Also as stated above SGD has the ability to get out of local minimas because of the fact that it can overlook the steepest step of the training sample as a whole and \"mistakenly\" step out of the local minima. \n",
    "\n",
    "**Answer: 4**  \n",
    "An instance where GD is not usable at all is when the data set/ training set is very large and thus computationally practically impossible to compute the gradients. In cases like this mbSGD offers a good solution to optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** In general we would expect the model to converge faster meaning the number or iterations would decrease. A larger mini-batch size allows to calculate more representative gradients in the model and thus step in a more accurate direction to the minima. However, it is worth noting that if 2B is too large it can be similar to the issues that arise when implementing regular GD (getting stuck in local minimas and computational time). It is also worth noting that twice the ram doesn't necessarily mean that the computations will be twice as fast (or the same speed for a batch twice as large) there can be different bottlenecks that can effect computational time. In addition, it is worth noting that other hyperparameters should be tweaked in accordance to the new mini-batch size, like learning rate for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For each of the following statements, state whether they're **true or false** and explain why.\n",
    "  1. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset.\n",
    "  1. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD.\n",
    "  1. SGD is less likely to get stuck in local minima, compared to GD.\n",
    "  1. Training  with SGD requires more memory than with GD.\n",
    "  1. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum.\n",
    "  1. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "5. True: when using SGD the optimization step is calculated based of one single sample in the data set. mbSGD uses a specific batch size of samples that are shuffled between each iteration. This is specifically the advantage of SGD over GD we do not need to compute the step from all the data set. However an epoch if defined as an iteration through the whole training data set thus we will take a step for each sample.\n",
    "\n",
    "6. False: Since SGD uses a subset of the data the gradient calculated is less accurate and thus doesn't necessarily step in the most optimal direction. Usually GD will converge in less iterations but it will also require more computational time/power. SGD will thus have more variance and will usually require more steps to converge to the minima but it may take less real time due to the easier computations.\n",
    "\n",
    "7. True: Due the the fact that SGD uses a random subset (or single sample) to calculate the gradient it usually will not compute the steepest gradient and compute the next step direction that doesn't go directly to the minima, by so it can step out of/over a local minima. \n",
    "\n",
    "8. False: Due to the fact that GD uses the entire dataset to compute the gradient it needs to load the entire data set into the memory for the gradient calculation, whereas the SGD only loads a subset.\n",
    "\n",
    "9. False: Neither are guaranteed to converge to a global minima, if GD gets caught in a local minima it will not reach the global one. Even SGD is not guaranteed to converge to a local minima because it is dependant on the subset of samples that computes the gradient in each step, which can introduce noise. Theoretically the model can oscillate around the global or a local minima or saddle point. \n",
    "\n",
    "10. True: Generally, In the case of a narrow ravine SGD with momentum can utilize the momentum built in the direction of the ravine to avoid oscillations in and out of the ravine. Newtons method which uses second order approximations is prone to being caught in the mentioned situation and take less optimal steps due to it being more sensitive to the curvature of the loss field. Overall the momentum can guide and allow the optimization algorithm to speed up in the direction of the ravine and avoid oscillations and saddle points. However neither of these methods are promised to prevail over the other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In tutorial 5 we saw an example of bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "  **True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "  Provide a mathematical justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** False, when solving a bi-level optimization problem we can regard the out optimization problem as the main problem with respect to the inner problem, then in effect we can use the chain rule to solve through the entire problem. To use the chain rule we must ensure that the optimization method of the inner problem is differentiable, SGD and LBFGS are both examples of differentiable optimization methods. Some different methods are not differentiable, like simulated annealing. However if it is worth noting that it can be solved without these methods if we can formulate a closed solution of the gradient, this is a vary rare and impractical solution though. Thus the answer is technically false but in a practical sense true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$.\n",
    "  Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers.\n",
    "  1. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\".\n",
    "  2. How can each of these problems be caused by increased depth?\n",
    "  3. Provide a numerical example demonstrating each.\n",
    "  4. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer::** \n",
    "\n",
    "7. Vanishing and exploding gradients are when the gradients of the lower levels of a network become very small or very large respectfully. In the case of vanishing gradients the gradient become so small that the weights of the lower levels do not update during back propagation, in contrast the exploding gradient problem occurs when the gradient are so large that the weights update in a away that diverges and the model becomes unstable.\n",
    "\n",
    "8. Since the vanishing/exploding gradient problems occur due to the gradient approaching 0 or infinity and being backpropagated through the layers, as we increase the depth of the model we increase the number of updates of the gradient which in return can amplify large or small gradients. If in a small network a \"bad\" gradient may only propagate through a few layers, in a large network it may have a massive impact on the entire networks optimization due to the exponential amplification that occurs and its great effect on early layers.\n",
    "\n",
    "9. Numerical Examples:\n",
    "\n",
    "-Vanishing Gradient:\n",
    "If we take a deep network lets say with $n$ layers, use the sigmoid activation function and use randomly initiated weights. We know that the maximum gradient for each layer is $0.25$ and then if we back propagate we get that the gradient $\\nabla$ of layer $i$ is equal to $\\nabla(y_i) \\le 0.25^{n-i}$ thus when n goes to infinity we get that the gradient goes to zero and vanishes.\n",
    "\n",
    "-Exploding Gradient:\n",
    "Lets take a twisted yet simple example which each layer applies a function that's gradient is equivalent to $\\nabla(y_i) = a$ and $a>1$ then when n goes to infinity we get an exploding gradient of $\\nabla(y_i) \\le a^{n-i} \\rightarrow \\infty$\n",
    "\n",
    "10. We can tell which problem it is based on the loss function during training. If we see that the loss function is stagnant we can assume we have a vanishing gradients because the model is no longer updating the weights matrices due to the gradients approaching 0. If the loss function is fluctuating but not diverging/improving we can assume that our issue is exploding gradients due to the model itself not diverging to a minima because the gradients that are being propagated are extremely large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $$\n",
    "  \\hat{y}^{(i)} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}^{(i)}+ \\vec{b}_1) + \\vec{b}_2\n",
    "  $$\n",
    "  Your wish to minimize the in-sample loss function is defined as\n",
    "  $$\n",
    "  L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_{F}^{2} + \\norm{\\mat{W}_2}_{F}^{2} \\right)\n",
    "  $$\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "  $$\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "Denote the input to the activation function $\\varphi$ as $z$ and denote $\\varphi'$ as the derivative of the chosen activation function. \n",
    "Note: Since we are using a binary cross entropy loss function it is likely that $\\varphi$ is the sigmoid function. \n",
    "\n",
    "For further use to develope expressions based of the chain rule let's develop the following expression:\n",
    "\n",
    "$$\\frac{\\partial \\ell(y^{(i)},\\hat{y}^{(i)})}{\\partial \\hat{y}^{(i)}}=-\\frac{y}{\\hat{y}}+\\frac{(1-y)}{(1-\\hat{y})}\\\\\n",
    "=\\frac{-y(1-\\hat{y})+\\hat{y}(1-y)}{\\hat{y}(1-\\hat{y})} = \\frac{-y+y\\hat{y}+\\hat{y}-\\hat{y}y}{\\hat{y}(1-\\hat{y})}\\\\\n",
    "=\\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})}$$\n",
    "\n",
    "Now using the chain rule we can develope the required partial derivatives:\n",
    "\n",
    "- $\\frac{\\partial L_\\mathcal{S}}{\\partial W_{1}}$:\n",
    "    - $\\frac{\\partial\\hat{y}^{(i)}}{\\partial \\mat{W_1}} = \\frac{\\partial(\\mat{W_2}\\varphi(\\mat{W_1}x^{i}+b_1)+b_2)}{\\partial\\mat{W_1}}=\\mat{W_2}\\varphi'(\\mat{W_1}x^{i}+b_1)\\cdot x $ \n",
    "    \n",
    "    - $\\frac{\\partial L_\\mathcal{S}}{\\partial W_{1}} = \\frac{\\partial(\\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)})+\\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_{F}^{2} + \\norm{\\mat{W}_2}_{F}^{2} \\right))}{\\partial W_1} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\partial(\\ell(y^{(i)},\\hat{y}^{(i)}))}{\\partial\\hat{y}^{(i)}}\\cdot\\frac{\\partial\\hat{y}^{(i)}}{\\partial W_1} + \\lambda W_1 = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\hat{y}^{(i)}-y^{(i)}}{\\hat{y}^{(i)}(1-\\hat{y}^{(i)})}\\mat{W_2}x^i\\varphi'(\\mat{W_1}x^{i}+b_1) + \\lambda W_1$ \n",
    "  \n",
    "    \n",
    "        \n",
    "\n",
    "- $\\frac{\\partial L}{\\partial W_{2}}$:\n",
    "    - $\\frac{\\partial\\hat{y}^{(i)}}{\\partial \\mat{W_2}} = \\frac{\\partial(\\mat{W_2}\\varphi(\\mat{W_1}x^{i}+b_1)+b_2)}{\\partial\\mat{W_2}}=\\varphi'(\\mat{W_1}x^{i}+b_1)$ \n",
    "    \n",
    "    - $\\frac{\\partial L_\\mathcal{S}}{\\partial W_{2}} = \\frac{\\partial(\\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)})+\\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_{F}^{2} + \\norm{\\mat{W}_2}_{F}^{2} \\right))}{\\partial W_2} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\partial(\\ell(y^{(i)},\\hat{y}^{(i)}))}{\\partial\\hat{y}^{(i)}}\\cdot\\frac{\\partial\\hat{y}^{(i)}}{\\partial W_2} + \\lambda W_2 = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\hat{y}^{(i)}-y^{(i)}}{\\hat{y}^{(i)}(1-\\hat{y}^{(i)})}\\varphi(\\mat{W_1}x^{i}+b_1) + \\lambda W_2$ \n",
    "\n",
    "- $\\frac{\\partial L}{\\partial b_{1}}$:\n",
    "    - $\\frac{\\partial z^{(i)}}{\\partial b_1} = \\frac{\\partial\\mat{W_1}x^{(i)}+b_1}{\\partial\\mat{b_1}}=1$\n",
    "\n",
    "    - $\\frac{\\partial L_\\mathcal{S}}{\\partial b_{1}} = \\frac{\\partial(\\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)})+\\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_{F}^{2} + \\norm{\\mat{W}_2}_{F}^{2} \\right))}{\\partial b_1} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\partial(\\ell(y^{(i)},\\hat{y}^{(i)}))}{\\partial\\hat{y}^{(i)}}\\cdot\\frac{\\partial\\hat{y}^{(i)}}{\\partial z^{(i)}}\\cdot \\frac{\\partial z^{(i)}}{\\partial b_1} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\hat{y}^{(i)}-y^{(i)}}{\\hat{y}^{(i)}(1-\\hat{y}^{(i)})}\\mat{W_2}\\varphi'(z)$ \n",
    "    \n",
    "\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial b_{2}}$:\n",
    "    - $\\frac{\\partial\\hat{y}^{(i)}}{\\partial b_2} = \\frac{\\partial\\mat{W_2}\\varphi(\\mat{W_1}x^{(i)}+b_1)+b_2}{\\partial\\mat{b_2}}=1$\n",
    "\n",
    "    - $\\frac{\\partial L_\\mathcal{S}}{\\partial b_{2}} = \\frac{\\partial(\\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)})+\\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_{F}^{2} + \\norm{\\mat{W}_2}_{F}^{2} \\right))}{\\partial b_2} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\partial(\\ell(y^{(i)},\\hat{y}^{(i)}))}{\\partial\\hat{y}^{(i)}}\\cdot\\frac{\\partial\\hat{y}^{(i)}}{\\partial b_2} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\hat{y}^{(i)}-y^{(i)}}{\\hat{y}^{(i)}(1-\\hat{y}^{(i)})}$ \n",
    "\n",
    "- $\\frac{\\partial L}{\\partial x}$:\n",
    "    - $\\frac{\\partial\\hat{y}^{(i)}}{\\partial x^{(i)}} = \\frac{\\partial\\mat{W_2}\\varphi(\\mat{W_1}x^{(i)}+b_1)+b_2}{\\partial\\mat{x^{(i)}}}=\\mat{W_2}\\mat{W_1}\\cdot\\varphi'(\\mat{W_1}x^{(i)}+b_1)$\n",
    "\n",
    "    - $\\frac{\\partial L_\\mathcal{S}}{\\partial x^{(i)}} = \\frac{\\partial(\\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)})+\\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_{F}^{2} + \\norm{\\mat{W}_2}_{F}^{2} \\right))}{\\partial x^{(i)}} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\partial(\\ell(y^{(i)},\\hat{y}^{(i)}))}{\\partial\\hat{y}^{(i)}}\\cdot\\frac{\\partial\\hat{y}^{(i)}}{\\partial x^{(i)}} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{\\hat{y}^{(i)}-y^{(i)}}{\\hat{y}^{(i)}(1-\\hat{y}^{(i)})}\\mat{W_2}\\mat{W_1}\\cdot\\varphi'(\\mat{W_1}x^{(i)}+b_1)$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The derivative of a function $f(\\vec{x})$ at a point $\\vec{x}_0$ is\n",
    "  $$\n",
    "  f'(\\vec{x}_0)=\\lim_{\\Delta\\vec{x}\\to 0} \\frac{f(\\vec{x}_0+\\Delta\\vec{x})-f(\\vec{x}_0)}{\\Delta\\vec{x}}\n",
    "  $$\n",
    "  \n",
    "  1. Explain how this formula can be used in order to compute gradients of neural network parameters numerically, without automatic differentiation (AD).\n",
    "  \n",
    "  2. What are the drawbacks of this approach? List at least two drawbacks compared to AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "1. Let $\\vec{\\theta}$ be a vector consisting of all the variables in the network. Specifically we can denote each individual variable in the network as $\\theta _i$ and let $N$ be the number of variables in the network.\n",
    "\n",
    "Then we can define the gradient of the loss function $L$ w.r.t. each parameter as follows: \n",
    "$$\\nabla L_{\\vec{\\theta}}=(\\frac{\\partial{L}}{\\partial{\\theta _1}},...,\\frac{\\partial{L}}{\\partial{\\theta _N}})$$\n",
    "\n",
    "Using the provided equation to calculate the partial derivative of each parameter to then obtain the gradient of the function. i.e.\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\theta _i}} = \\lim_{\\Delta\\vec{\\theta}\\to 0} \\frac{L(\\vec{\\theta}+\\Delta{\\theta _i})-L(\\vec{\\theta})}{\\Delta\\vec{\\theta}}$$\n",
    "\n",
    "Then we can use the gradient to define the direction to step in based off of our optimization algorithm. \n",
    "\n",
    "2. The drawbacks of this approach are first that this is computationally taxing. In order to obtain this we need to compute the loss function for each parameter. \n",
    "\n",
    "In addition the method can be inaccurate. First, not all loss functions are differentiable at all points of their domain meaning that the method could give imprecise results. Also, this method requires to choose a value $\\Delta$ as a parameter. It can be difficult to choose the right magnitude for this parameter, if it's too small the gradient may suffer from rounding errors and if it is too large we will loose accuracy of the gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Given the following code snippet:\n",
    "  1. Write a short snippet that implements that calculates gradient of `loss` w.r.t. `W` and `b` using the approach of numerical gradients from the previous question.\n",
    "  2. Calculate the same derivatives with autograd.\n",
    "  3. Show, by calling `torch.allclose()` that your numerical gradient is close to autograd's gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.7795, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, d = 100, 5\n",
    "dtype = torch.float64\n",
    "X = torch.rand(N, d, dtype=dtype)\n",
    "W, b = torch.rand(d, d, requires_grad=True, dtype=dtype), torch.rand(d, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def foo(W, b):\n",
    "    return torch.mean(X @ W + b)\n",
    "\n",
    "loss = foo(W, b)\n",
    "print(f\"{loss=}\")\n",
    "\n",
    "# TODO: Calculate gradients numerically for W and b\n",
    "delta = 1e-6\n",
    "grad_W = torch.zeros_like(W)\n",
    "grad_b = torch.zeros_like(b)\n",
    "\n",
    "# gradient w.r.t. W\n",
    "for i in range(grad_W.shape[0]):\n",
    "    for j in range(grad_W.shape[1]):\n",
    "        W_temp = W.clone()\n",
    "        W_temp[i,j] += delta\n",
    "        loss_temp = foo(W_temp, b)\n",
    "        grad_W[i,j] = (loss_temp-loss)/delta\n",
    "\n",
    "# gradient w.r.t. b\n",
    "for i in range(grad_b.shape[0]):\n",
    "    b_temp = b.clone()\n",
    "    b_temp[i] += delta\n",
    "    loss_temp = foo(W, b_temp)\n",
    "    grad_b[i] = (loss_temp-loss)/delta\n",
    "\n",
    "# TODO: Compare with autograd using torch.allclose()\n",
    "loss.backward()\n",
    "autograd_W = W.grad\n",
    "autograd_b = b.grad\n",
    "assert torch.allclose(grad_W, autograd_W)\n",
    "assert torch.allclose(grad_b, autograd_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding word embeddings:\n",
    "  1. Explain this term and why it's used in the context of a language model.\n",
    "  1. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding (i.e. trained directly on sequences of tokens)? If yes, what would be the consequence for the trained model? if no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: 2**\n",
    "\n",
    "Embedding is the process of encoding tokens/words into tensors/multi-dimensional vectors in a way that preserves the some of the word's semantic attributes. Generally, each dimension is attributed to some feature of the token. This is used in the context of a language model because using this method allows the model to utilize the semantic values and the context of the words to preform it's required tasks in a more precise way. Without word embedding the tokens are do not hold their relations to one another. \n",
    "\n",
    "\n",
    "**Answer: 3** \n",
    "\n",
    "Yes is can be trained without using embedding however it would probably not reach a desired result. To train the model without embedding we would likely use one-hot encoding like we did in the homework assignments. This would lead to two major drawbacks. \n",
    "\n",
    "First, the model would not be able to recognize and utilize the semantic attributes of the words. This would lead to a less effective model. We can remember from the assignments that when we built a text generator of a Shakespeare play the model made specific patterns similar to the play appear but with no logic behind the wording, similarly the model for semantic analysis would not find the logic in the reviews and would struggle greatly with new words or new sequences of recognized words. \n",
    "\n",
    "Secondly, the model would be inefficient. The input layer of a one-hot encoded model is the size of all the words in the batch multiplied by the size of the vocabulary (because of the one-hot encoding). In contrast, a model using word embedding has an input layer with the size of the dimension of the embedding vector multiplied by the number of words in the batch. This is usually substantially smaller since generally the vocabulary is much larger than the number of features we attribute to the embedding. However it is worth noting that this is not always this case and depends on the specific task the model is trying to solve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considering the following snippet, explain:\n",
    "  1. What does `Y` contain? why this output shape?\n",
    "  2. How you would implement `nn.Embedding` yourself using only torch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape=torch.Size([5, 6, 7, 8, 42000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "2.1. Y contains a tensor that holds all of the input values from X but in a dense vector/embedded tensor. There are 42 words in the vocabulary here so embedding holds a table with the corresponding embedding for each word in the vocabulary. Applying embedding on the input vector X in this case (5, 6, 7, 8) (which holds the sequences and batches) returns a corresponding tensor of the input shape X but with the multi-dimensional (42000) embedding of each token. Thus the output shape of (5, 6, 7, 8, 42000)\n",
    "\n",
    "2.2. It is possible to do it with a Matrix of size (num of embeddings, embedding dim) or (42,42000) like in the above example. And then when we want to get the value of a specific token we need to look up the input vector X in it using indexing (passing the input tensor as the index to the tensor we want to get the embedding from). Usually a common way to initialize the embedding matrix can be to use random values to ensure there is no predetermined relationships between words or use a pre-trained embedding matrix. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of S: State whether the following sentences are **true or false**, and explain.\n",
    "  1. TBPTT uses a modified version of the backpropagation algorithm.\n",
    "  2. To implement TBPTT we only need to limit the length of the sequence provided to the model to length S.\n",
    "  3. TBPTT allows the model to learn relations between input that are at most S timesteps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "3.1 True, the concept is similar to using mini-batch SGD. We calculate the gradient and backpropagate back through the model only by a defined number of layers. This helps minimize computational time and help deal with the vanishing/exploding gradient problems that often occur in sequence models due to them back propagating through time and thus the gradient for the loss can be dependant on many layers. \n",
    "\n",
    "3.2 False, we need to use the whole sequence and can even use an infinitely long sequence. The implementation is significant in that we update the gradient after a fixed length of layers. But we need the whole sequence to reach optimal results. \n",
    "\n",
    "3.3 False, the gradient is only calculated for up to S timestaps apart and thus each iteration only learns new relations that are at most S apart. However over multiple iterations, the model in whole can learn relations that are global. It does however hinder the models ability to provide accurate answers theoretically but in a practical setting it allows the model to reach better results since the input data may be so large that otherwise training is impossible and even when using truncation it picks up on local and global relations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In tutorial 7 (part 2) we learned how to use attention to perform alignment between a source and target sequence in machine translation.\n",
    "  1. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention?\n",
    "  \n",
    "  2. After learning that self-attention is gaining popularity thanks to the shiny new transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections). What influence do you expect this will have on the learned hidden states?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "1.1 The addition of attention allows the model to generate different and potentially more precise hidden states. The mechanism allows the decoder to use more precise/relevant information gained from the encoder through a hidden state that summarizes the text, the state is then used to better align the text. The decoder can regard a specific portion of the encoder hidden states instead of having to regard the entire sequence summarized into a single state by the encoder. This is a very intuitive approach as it is similar to how we would translate information; instead of first processing the information in its whole we would divide it into relevant parts and translate in a segmented manner.\n",
    "\n",
    "1.2 Using the encoders hidden states as the keys, queries and values of the model allows the model to attend all parts of the encoders output sequence as opposed to using queries from the decoder hidden states where the models attention is determined by the current decoder state. We can assume that this change will allow the learned hidden states to capture more distant relationships between tokens. Often in the decoder models use a both types of blocks to achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "What would be the qualitative effect of this on:\n",
    "\n",
    "  1. Images reconstructed by the model during training ($x\\to z \\to x'$)?\n",
    "  1. Images generated by the model ($z \\to x'$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "2. During training we would get better results. The model would work like a regular auto-encoder since the KL-divergence term is the factor that ensures the distribution that is being learned in the VAE is similar to a specified distribution. The reconstructed images in training would be more similar to the input images because the model would have \"overfit\" the learned dist. to the input data's dist. \n",
    "\n",
    "3. The generated images however would suffer significantly from this. The reconstructed images would appear less diverse, less clear and less similar to what we would expect. The latent space generated by the model would not follow a specific distribution and thus loose the ability to create images with diversity and expected features. The model learned a latent space in accordance to the input models and loose from the stochastic benefits of the VAE. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  2. If we feed the same image to the encoder multiple times, then decode each result, we'll get the same reconstruction.\n",
    "  3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "3. False, the loss function in a VAE consists of a reconstruction loss and KL-divergence. The model does not simply learn the normal or otherwise specified distribution as the latent-space but also aims to learn a distribution that also minimizes reconstruction error but is close to or \"follows\" the distribution given. Otherwise we would lose the meaningful information of the input that we aim to learn.\n",
    "\n",
    "4. False, the model has the stochastic element of sampling which is what allows it to generate different representations and is the underlying concept in the VAE. The model may then generate a different latent representation even for the same image, it is worth noting though that they will most likely hold similarities since the model still aims to minimize the reconstruction loss.\n",
    "\n",
    "5. True, the true loss is intractable because the KL-divergence which measures the difference between the learned and given distribution is often intractable due to the computation of the integrals needed to be calculated in the expression of $log(P(x))$. Thus the upper bound (ELBO) of the loss and specifically the KL-divergence can be regarded as an optimization problem in the model that is attempted to be limited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "  2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "  3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "  5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "3. False. When training a GAN we are essentially solving a min max problem: \n",
    "$$\\underset{G}{min} \\; \\underset{D}{max} \\; V(D,G)$$ \n",
    "Where,\n",
    "$$ V(D,G) = \\mathbb{E}_{xâˆ¼p(x)}[logD(x)]+\\mathbb{E}_{zâˆ¼q(z)}[log(1âˆ’D(G(z)))]$$\n",
    "Thus we do want the generators loss to be low and by this achieving a generator which is able to fool the discriminator and have a high loss to the discriminator indicating that the discriminator is indeed being fooled. However, a high loss of the discriminator is not necessarily good. We want a good discriminator model that preforms well in discerning between the synthesized and input images. Thus we do not want any model with a high discriminator loss. Optimally we would like to achieve a point which the discriminator is quessing randomly, i.e. being correct for half of the predictions, there can be a case with lower loss if it is always wrong. \n",
    "\n",
    "4. False. When training the discriminator (D) we do not want to update the generator's weights thus do not want to backprogate through it. When training the D we are in some sort providing feedback to the generator (G) and thus if we were to backprop through it and update its weights with regard to the D's training we would be \"overfitting\" the model and loose the adversarial component of the network. \n",
    " \n",
    "5. True. In most GAN implementations the network takes an input of a latent vector and generates an image from it and $\\mathcal{N}(\\vec{0},\\vec{I})$ can be that latent space, specifically if it was we trained on. \n",
    "\n",
    "6. True. Usually the discriminator is initialized randomly and thus in the first epochs of training it does not provide any beneficial feedback to the generator. Thus by training for a few epochs prior to training the G is often beneficial to ensure that the training process more efficient and that the D is able to discern between synthesized and real input before training the G.\n",
    "\n",
    "7. False. Once reaching a steady state of 50% the generator has \"beaten\" the discriminator, for the discriminator could be randomly guessing real/fake and reach 50 percent. Thus further training will not improve the loss/accuracy but, it still may improve the images quality or features/details it generates. This is usually not the case though and the improvements will most likely be minimal so the answer is False. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "db2aeb4a2d20d8f60ca10a95a693e4fbc8d4e86da5b1f77d6fc445990905a603"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
